{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instructions\n",
    "\n",
    "You are asked to complete the following files:\n",
    "* **pruned_layers.py**, which contains the pruning of DNNs to reduce the storage of insignificant weight parameters with 2 methods: pruning by percentage and prune by standard deviation.\n",
    "* **train_util.py**, which includes the training process of DNNs with pruned connections.\n",
    "* **quantize.py**, which applies the quantization (weight sharing) part on the DNN to reduce the storage of weight parameters.\n",
    "* **huffman_coding.py**, which applies the Huffman coding onto the weight of DNNs to further compress the weight size.\n",
    "\n",
    "You are asked to submit the following files:\n",
    "* **net_before_pruning.pt**, which is the set of weight parameters before applying pruning on DNN weight parameters.\n",
    "* **net_after_pruning.pt**, which is the set of weight paramters after applying pruning on DNN weight parameters.\n",
    "* **net_after_quantization.pt**, which is the set of weight parameters after applying quantization (weight sharing) on DNN weight parameters.\n",
    "* **codebook_resnet20.npy**, which is the quantization codebook of each layer after applying quantization (weight sharing).\n",
    "* **huffman_encoding.npy**, which is the encoding map of each item within the quantization codebook in the whole DNN architecture.\n",
    "* **huffman_freq.npy**, which is the frequency map of each item within the quantization codebook in the whole DNN. \n",
    "\n",
    "To ensure fair grading policy, we fix the choice of model to ResNet-20. You may check the implementation in **resnet20.py** for more details."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2020-09-30T03:30:25.835449Z",
     "start_time": "2020-09-30T03:30:21.314583Z"
    },
    "pycharm": {
     "is_executing": false
    }
   },
   "outputs": [],
   "source": [
    "from resnet20 import ResNetCIFAR\n",
    "from train_util import train, finetune_after_prune, test\n",
    "from quantize import quantize_whole_model\n",
    "from huffman_coding import huffman_coding\n",
    "from summary import summary\n",
    "import torch\n",
    "import numpy as np\n",
    "from prune import prune\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Full-precision model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-30T03:30:21.313Z"
    },
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "net = ResNetCIFAR(num_layers=20)\n",
    "net = net.to(device)\n",
    "\n",
    "# Uncomment to load pretrained weights\n",
    "# net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n",
    "# Comment if you have loaded pretrained weights\n",
    "# train(net, epochs=100, batch_size=128, lr=0.01, reg=1e-2)\n",
    "# train(net, epochs=100, batch_size=128, lr=0.01, reg=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-30T03:30:21.316Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Files already downloaded and verified\nTest Loss=0.2586, Test accuracy=0.9168\n"
    }
   ],
   "source": [
    "# Load the best weight paramters\n",
    "net.load_state_dict(torch.load(\"net_before_pruning.pt\"))\n",
    "test(net)\n",
    "\n",
    "# train(net, epochs=10, batch_size=128, lr=0.01, reg=1e-2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-30T03:30:21.320Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-----Summary before pruning-----\nLayer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n1\t\tConvolutional\t864\t\t218\t\t\t0.747685\n2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n4\t\tConvolutional\t4608\t\t1120\t\t\t0.756944\n5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n7\t\tConvolutional\t2304\t\t839\t\t\t0.635851\n8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n9\t\tConvolutional\t512\t\t143\t\t\t0.720703\n10\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n11\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n12\t\tConvolutional\t2304\t\t829\t\t\t0.640191\n13\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n14\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n15\t\tConvolutional\t2304\t\t852\t\t\t0.630208\n16\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n17\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n18\t\tConvolutional\t2304\t\t806\t\t\t0.650174\n19\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n20\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n21\t\tConvolutional\t2304\t\t763\t\t\t0.668837\n22\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n23\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n24\t\tConvolutional\t4608\t\t1754\t\t\t0.619358\n25\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n26\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n27\t\tConvolutional\t9216\t\t3696\t\t\t0.598958\n28\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n29\t\tConvolutional\t512\t\t164\t\t\t0.679688\n30\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n31\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n32\t\tConvolutional\t9216\t\t3743\t\t\t0.593859\n33\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n34\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n35\t\tConvolutional\t9216\t\t3796\t\t\t0.588108\n36\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n37\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n38\t\tConvolutional\t9216\t\t3617\t\t\t0.607530\n39\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n40\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n41\t\tConvolutional\t9216\t\t3554\t\t\t0.614366\n42\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n43\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n44\t\tConvolutional\t18432\t\t7607\t\t\t0.587294\n45\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n46\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n47\t\tConvolutional\t36864\t\t15130\t\t\t0.589572\n48\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n49\t\tConvolutional\t2048\t\t823\t\t\t0.598145\n50\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n51\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n52\t\tConvolutional\t36864\t\t14655\t\t\t0.602458\n53\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n54\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n55\t\tConvolutional\t36864\t\t10750\t\t\t0.708388\n56\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n57\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n58\t\tConvolutional\t36864\t\t13473\t\t\t0.634521\n59\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n60\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n61\t\tConvolutional\t36864\t\t14645\t\t\t0.602729\n62\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n63\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n64\t\tLinear\t\t640\t\t110\t\t\t0.828125\nTotal nonzero parameters: 103087\nTotal parameters: 274144\nTotal sparsity: 0.623968\n-------------------------------\n"
    }
   ],
   "source": [
    "print(\"-----Summary before pruning-----\")\n",
    "summary(net)\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pruning & Finetune with pruned connections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-30T03:30:21.323Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Files already downloaded and verified\nTest Loss=1.1101, Test accuracy=0.6618\n"
    }
   ],
   "source": [
    "# Test accuracy before fine-tuning\n",
    "prune(net, method='std', q=0.0, s=0.8)\n",
    "test(net)\n",
    "# print(net.sparsity)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-30T03:30:21.331Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "==> Preparing data..\nFiles already downloaded and verified\nFiles already downloaded and verified\n\nEpoch: 0\n[Step=50]\tLoss=0.3529\tacc=0.9114\t3057.0 examples/second\n[Step=100]\tLoss=0.3596\tacc=0.9086\t6449.7 examples/second\n[Step=150]\tLoss=0.3532\tacc=0.9110\t6622.8 examples/second\n[Step=200]\tLoss=0.3448\tacc=0.9139\t6665.6 examples/second\n[Step=250]\tLoss=0.3394\tacc=0.9154\t6702.7 examples/second\n[Step=300]\tLoss=0.3339\tacc=0.9176\t6727.8 examples/second\n[Step=350]\tLoss=0.3290\tacc=0.9191\t6728.8 examples/second\nTest Loss=0.3592, Test acc=0.8951\nSaving...\n\nEpoch: 1\n[Step=400]\tLoss=0.2973\tacc=0.9280\t1822.6 examples/second\n[Step=450]\tLoss=0.3034\tacc=0.9224\t6782.2 examples/second\n[Step=500]\tLoss=0.3030\tacc=0.9237\t6803.8 examples/second\n[Step=550]\tLoss=0.3003\tacc=0.9258\t6668.4 examples/second\n[Step=600]\tLoss=0.2982\tacc=0.9268\t6802.9 examples/second\n[Step=650]\tLoss=0.2959\tacc=0.9269\t6859.6 examples/second\n[Step=700]\tLoss=0.2949\tacc=0.9270\t6793.0 examples/second\n[Step=750]\tLoss=0.2941\tacc=0.9273\t6781.9 examples/second\nTest Loss=0.3421, Test acc=0.8964\nSaving...\n\nEpoch: 2\n[Step=800]\tLoss=0.2879\tacc=0.9275\t1777.1 examples/second\n[Step=850]\tLoss=0.2784\tacc=0.9313\t6730.5 examples/second\n[Step=900]\tLoss=0.2788\tacc=0.9307\t6303.7 examples/second\n[Step=950]\tLoss=0.2798\tacc=0.9302\t6548.6 examples/second\n[Step=1000]\tLoss=0.2780\tacc=0.9309\t6845.3 examples/second\n[Step=1050]\tLoss=0.2776\tacc=0.9318\t6780.2 examples/second\n[Step=1100]\tLoss=0.2784\tacc=0.9312\t6769.4 examples/second\n[Step=1150]\tLoss=0.2775\tacc=0.9313\t6786.5 examples/second\nTest Loss=0.3336, Test acc=0.8975\nSaving...\n\nEpoch: 3\n[Step=1200]\tLoss=0.2729\tacc=0.9332\t1792.9 examples/second\n[Step=1250]\tLoss=0.2716\tacc=0.9336\t6741.2 examples/second\n[Step=1300]\tLoss=0.2676\tacc=0.9351\t6773.2 examples/second\n[Step=1350]\tLoss=0.2687\tacc=0.9351\t6870.2 examples/second\n[Step=1400]\tLoss=0.2683\tacc=0.9357\t6865.1 examples/second\n[Step=1450]\tLoss=0.2684\tacc=0.9352\t6830.2 examples/second\n[Step=1500]\tLoss=0.2704\tacc=0.9337\t6818.7 examples/second\n[Step=1550]\tLoss=0.2702\tacc=0.9335\t6852.4 examples/second\nTest Loss=0.3271, Test acc=0.9006\nSaving...\n\nEpoch: 4\n[Step=1600]\tLoss=0.2690\tacc=0.9321\t1782.6 examples/second\n[Step=1650]\tLoss=0.2621\tacc=0.9355\t6745.8 examples/second\n[Step=1700]\tLoss=0.2652\tacc=0.9354\t6867.4 examples/second\n[Step=1750]\tLoss=0.2637\tacc=0.9359\t6835.4 examples/second\n[Step=1800]\tLoss=0.2635\tacc=0.9358\t6822.9 examples/second\n[Step=1850]\tLoss=0.2630\tacc=0.9357\t6833.7 examples/second\n[Step=1900]\tLoss=0.2624\tacc=0.9358\t6810.6 examples/second\n[Step=1950]\tLoss=0.2628\tacc=0.9352\t6871.0 examples/second\nTest Loss=0.3230, Test acc=0.9013\nSaving...\n\nEpoch: 5\n[Step=2000]\tLoss=0.2746\tacc=0.9302\t1776.2 examples/second\n[Step=2050]\tLoss=0.2645\tacc=0.9355\t6778.6 examples/second\n[Step=2100]\tLoss=0.2628\tacc=0.9359\t6845.1 examples/second\n[Step=2150]\tLoss=0.2620\tacc=0.9353\t6821.6 examples/second\n[Step=2200]\tLoss=0.2600\tacc=0.9366\t6766.8 examples/second\n[Step=2250]\tLoss=0.2581\tacc=0.9377\t6774.8 examples/second\n[Step=2300]\tLoss=0.2587\tacc=0.9368\t6810.1 examples/second\nTest Loss=0.3205, Test acc=0.9018\nSaving...\n\nEpoch: 6\n[Step=2350]\tLoss=0.2486\tacc=0.9434\t1815.4 examples/second\n[Step=2400]\tLoss=0.2588\tacc=0.9361\t6627.7 examples/second\n[Step=2450]\tLoss=0.2571\tacc=0.9361\t6686.1 examples/second\n[Step=2500]\tLoss=0.2565\tacc=0.9366\t6819.2 examples/second\n[Step=2550]\tLoss=0.2549\tacc=0.9368\t6801.8 examples/second\n[Step=2600]\tLoss=0.2578\tacc=0.9354\t6753.7 examples/second\n[Step=2650]\tLoss=0.2582\tacc=0.9353\t6701.4 examples/second\n[Step=2700]\tLoss=0.2575\tacc=0.9354\t6892.2 examples/second\nTest Loss=0.3199, Test acc=0.9020\nSaving...\n\nEpoch: 7\n[Step=2750]\tLoss=0.2215\tacc=0.9423\t1812.3 examples/second\n[Step=2800]\tLoss=0.2363\tacc=0.9428\t6784.1 examples/second\n[Step=2850]\tLoss=0.2462\tacc=0.9399\t6645.6 examples/second\n[Step=2900]\tLoss=0.2458\tacc=0.9407\t6824.7 examples/second\n[Step=2950]\tLoss=0.2446\tacc=0.9413\t6807.3 examples/second\n[Step=3000]\tLoss=0.2502\tacc=0.9392\t6509.1 examples/second\n[Step=3050]\tLoss=0.2497\tacc=0.9392\t6762.1 examples/second\n[Step=3100]\tLoss=0.2504\tacc=0.9390\t6824.6 examples/second\nTest Loss=0.3144, Test acc=0.9036\nSaving...\n\nEpoch: 8\n[Step=3150]\tLoss=0.2444\tacc=0.9453\t1801.8 examples/second\n[Step=3200]\tLoss=0.2478\tacc=0.9388\t6812.5 examples/second\n[Step=3250]\tLoss=0.2509\tacc=0.9368\t6736.0 examples/second\n[Step=3300]\tLoss=0.2502\tacc=0.9374\t6705.5 examples/second\n[Step=3350]\tLoss=0.2496\tacc=0.9379\t6637.4 examples/second\n[Step=3400]\tLoss=0.2503\tacc=0.9379\t6732.8 examples/second\n[Step=3450]\tLoss=0.2501\tacc=0.9382\t6805.6 examples/second\n[Step=3500]\tLoss=0.2500\tacc=0.9381\t6783.3 examples/second\nTest Loss=0.3140, Test acc=0.9033\n\nEpoch: 9\n[Step=3550]\tLoss=0.2438\tacc=0.9393\t1815.8 examples/second\n[Step=3600]\tLoss=0.2452\tacc=0.9391\t6765.2 examples/second\n[Step=3650]\tLoss=0.2476\tacc=0.9375\t6713.6 examples/second\n[Step=3700]\tLoss=0.2466\tacc=0.9386\t6625.9 examples/second\n[Step=3750]\tLoss=0.2465\tacc=0.9382\t6707.3 examples/second\n[Step=3800]\tLoss=0.2455\tacc=0.9390\t6742.2 examples/second\n[Step=3850]\tLoss=0.2482\tacc=0.9381\t6723.3 examples/second\n[Step=3900]\tLoss=0.2463\tacc=0.9390\t6793.5 examples/second\nTest Loss=0.3105, Test acc=0.9036\n\nEpoch: 10\n[Step=3950]\tLoss=0.2454\tacc=0.9385\t1747.1 examples/second\n[Step=4000]\tLoss=0.2419\tacc=0.9409\t6642.6 examples/second\n[Step=4050]\tLoss=0.2419\tacc=0.9408\t6674.5 examples/second\n[Step=4100]\tLoss=0.2433\tacc=0.9402\t6792.6 examples/second\n[Step=4150]\tLoss=0.2440\tacc=0.9396\t6804.3 examples/second\n[Step=4200]\tLoss=0.2433\tacc=0.9402\t6808.8 examples/second\n[Step=4250]\tLoss=0.2440\tacc=0.9405\t6790.3 examples/second\n[Step=4300]\tLoss=0.2431\tacc=0.9407\t6720.4 examples/second\nTest Loss=0.3099, Test acc=0.9034\n\nEpoch: 11\n[Step=4350]\tLoss=0.2480\tacc=0.9373\t1820.4 examples/second\n[Step=4400]\tLoss=0.2421\tacc=0.9406\t6691.1 examples/second\n[Step=4450]\tLoss=0.2423\tacc=0.9401\t6763.6 examples/second\n[Step=4500]\tLoss=0.2407\tacc=0.9420\t6784.4 examples/second\n[Step=4550]\tLoss=0.2395\tacc=0.9421\t6813.2 examples/second\n[Step=4600]\tLoss=0.2400\tacc=0.9416\t6637.9 examples/second\n[Step=4650]\tLoss=0.2407\tacc=0.9411\t6779.5 examples/second\nTest Loss=0.3092, Test acc=0.9041\nSaving...\n\nEpoch: 12\n[Step=4700]\tLoss=0.2312\tacc=0.9375\t1762.5 examples/second\n[Step=4750]\tLoss=0.2402\tacc=0.9398\t6699.4 examples/second\n[Step=4800]\tLoss=0.2357\tacc=0.9436\t6671.1 examples/second\n[Step=4850]\tLoss=0.2385\tacc=0.9431\t6579.9 examples/second\n[Step=4900]\tLoss=0.2386\tacc=0.9434\t6745.5 examples/second\n[Step=4950]\tLoss=0.2393\tacc=0.9421\t6649.0 examples/second\n[Step=5000]\tLoss=0.2413\tacc=0.9412\t6732.1 examples/second\n[Step=5050]\tLoss=0.2404\tacc=0.9418\t6719.9 examples/second\nTest Loss=0.3067, Test acc=0.9042\nSaving...\n\nEpoch: 13\n[Step=5100]\tLoss=0.2187\tacc=0.9499\t1769.9 examples/second\n[Step=5150]\tLoss=0.2373\tacc=0.9434\t6853.2 examples/second\n[Step=5200]\tLoss=0.2403\tacc=0.9412\t6622.3 examples/second\n[Step=5250]\tLoss=0.2393\tacc=0.9424\t6578.0 examples/second\n[Step=5300]\tLoss=0.2381\tacc=0.9431\t6869.7 examples/second\n[Step=5350]\tLoss=0.2370\tacc=0.9432\t6800.0 examples/second\n[Step=5400]\tLoss=0.2372\tacc=0.9426\t6733.4 examples/second\n[Step=5450]\tLoss=0.2378\tacc=0.9424\t6810.7 examples/second\nTest Loss=0.3046, Test acc=0.9048\nSaving...\n\nEpoch: 14\n[Step=5500]\tLoss=0.2446\tacc=0.9399\t1858.9 examples/second\n[Step=5550]\tLoss=0.2331\tacc=0.9420\t6803.1 examples/second\n[Step=5600]\tLoss=0.2365\tacc=0.9409\t6770.2 examples/second\n[Step=5650]\tLoss=0.2352\tacc=0.9426\t6756.5 examples/second\n[Step=5700]\tLoss=0.2380\tacc=0.9411\t6762.8 examples/second\n[Step=5750]\tLoss=0.2366\tacc=0.9419\t6812.1 examples/second\n[Step=5800]\tLoss=0.2380\tacc=0.9412\t6766.6 examples/second\n[Step=5850]\tLoss=0.2381\tacc=0.9414\t6719.6 examples/second\nTest Loss=0.3052, Test acc=0.9045\n\nEpoch: 15\n[Step=5900]\tLoss=0.2273\tacc=0.9475\t1765.7 examples/second\n[Step=5950]\tLoss=0.2423\tacc=0.9387\t6628.1 examples/second\n[Step=6000]\tLoss=0.2349\tacc=0.9410\t6684.5 examples/second\n[Step=6050]\tLoss=0.2342\tacc=0.9419\t6706.4 examples/second\n[Step=6100]\tLoss=0.2330\tacc=0.9433\t6740.5 examples/second\n[Step=6150]\tLoss=0.2354\tacc=0.9422\t6768.1 examples/second\n[Step=6200]\tLoss=0.2356\tacc=0.9424\t6804.2 examples/second\n[Step=6250]\tLoss=0.2357\tacc=0.9421\t6809.1 examples/second\nTest Loss=0.3061, Test acc=0.9048\n\nEpoch: 16\n[Step=6300]\tLoss=0.2290\tacc=0.9448\t1700.8 examples/second\n[Step=6350]\tLoss=0.2330\tacc=0.9431\t6670.4 examples/second\n[Step=6400]\tLoss=0.2360\tacc=0.9419\t6780.8 examples/second\n[Step=6450]\tLoss=0.2350\tacc=0.9426\t6736.7 examples/second\n[Step=6500]\tLoss=0.2333\tacc=0.9436\t6740.7 examples/second\n[Step=6550]\tLoss=0.2347\tacc=0.9425\t6763.7 examples/second\n[Step=6600]\tLoss=0.2332\tacc=0.9432\t6807.5 examples/second\nTest Loss=0.3033, Test acc=0.9056\nSaving...\n\nEpoch: 17\n[Step=6650]\tLoss=0.2158\tacc=0.9453\t1824.9 examples/second\n[Step=6700]\tLoss=0.2310\tacc=0.9446\t6598.6 examples/second\n[Step=6750]\tLoss=0.2384\tacc=0.9422\t6661.4 examples/second\n[Step=6800]\tLoss=0.2337\tacc=0.9430\t6588.4 examples/second\n[Step=6850]\tLoss=0.2338\tacc=0.9431\t6570.9 examples/second\n[Step=6900]\tLoss=0.2353\tacc=0.9426\t6482.0 examples/second\n[Step=6950]\tLoss=0.2357\tacc=0.9425\t6683.9 examples/second\n[Step=7000]\tLoss=0.2362\tacc=0.9422\t6753.7 examples/second\nTest Loss=0.3013, Test acc=0.9054\n\nEpoch: 18\n[Step=7050]\tLoss=0.2596\tacc=0.9297\t1780.9 examples/second\n[Step=7100]\tLoss=0.2333\tacc=0.9417\t6729.7 examples/second\n[Step=7150]\tLoss=0.2301\tacc=0.9436\t6463.8 examples/second\n[Step=7200]\tLoss=0.2324\tacc=0.9430\t6698.3 examples/second\n[Step=7250]\tLoss=0.2328\tacc=0.9434\t6761.0 examples/second\n[Step=7300]\tLoss=0.2316\tacc=0.9436\t6780.3 examples/second\n[Step=7350]\tLoss=0.2313\tacc=0.9436\t6574.1 examples/second\n[Step=7400]\tLoss=0.2312\tacc=0.9437\t6806.9 examples/second\nTest Loss=0.3041, Test acc=0.9040\n\nEpoch: 19\n[Step=7450]\tLoss=0.2511\tacc=0.9364\t1854.4 examples/second\n[Step=7500]\tLoss=0.2369\tacc=0.9422\t6771.5 examples/second\n[Step=7550]\tLoss=0.2366\tacc=0.9417\t6794.5 examples/second\n[Step=7600]\tLoss=0.2307\tacc=0.9436\t6807.0 examples/second\n[Step=7650]\tLoss=0.2312\tacc=0.9431\t6801.2 examples/second\n[Step=7700]\tLoss=0.2303\tacc=0.9435\t6742.9 examples/second\n[Step=7750]\tLoss=0.2314\tacc=0.9428\t6733.4 examples/second\n[Step=7800]\tLoss=0.2323\tacc=0.9424\t6705.6 examples/second\nTest Loss=0.3032, Test acc=0.9047\n"
    }
   ],
   "source": [
    "# Uncomment to load pretrained weights\n",
    "# net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "# Comment if you have loaded pretrained weights\n",
    "finetune_after_prune(net, epochs=20, batch_size=128, lr=0.00001, reg=1e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-30T03:30:21.336Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Files already downloaded and verified\nTest Loss=0.3033, Test accuracy=0.9056\n"
    }
   ],
   "source": [
    "# Load the best weight paramters\n",
    "net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "test(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-30T03:30:21.340Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "-----Summary After pruning-----\nLayer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n1\t\tConvolutional\t864\t\t492\t\t\t0.430556\n2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n4\t\tConvolutional\t4608\t\t2633\t\t\t0.428602\n5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n7\t\tConvolutional\t2304\t\t1768\t\t\t0.232639\n8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n9\t\tConvolutional\t512\t\t313\t\t\t0.388672\n10\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n11\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n12\t\tConvolutional\t2304\t\t1742\t\t\t0.243924\n13\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n14\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n15\t\tConvolutional\t2304\t\t1760\t\t\t0.236111\n16\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n17\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n18\t\tConvolutional\t2304\t\t1637\t\t\t0.289497\n19\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n20\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n21\t\tConvolutional\t2304\t\t1642\t\t\t0.287326\n22\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n23\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n24\t\tConvolutional\t4608\t\t3612\t\t\t0.216146\n25\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n26\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n27\t\tConvolutional\t9216\t\t7338\t\t\t0.203776\n28\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n29\t\tConvolutional\t512\t\t385\t\t\t0.248047\n30\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n31\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n32\t\tConvolutional\t9216\t\t7366\t\t\t0.200738\n33\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n34\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n35\t\tConvolutional\t9216\t\t7324\t\t\t0.205295\n36\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n37\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n38\t\tConvolutional\t9216\t\t6878\t\t\t0.253689\n39\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n40\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n41\t\tConvolutional\t9216\t\t6827\t\t\t0.259223\n42\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n43\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n44\t\tConvolutional\t18432\t\t14721\t\t\t0.201335\n45\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n46\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n47\t\tConvolutional\t36864\t\t29453\t\t\t0.201036\n48\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n49\t\tConvolutional\t2048\t\t1616\t\t\t0.210938\n50\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n51\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n52\t\tConvolutional\t36864\t\t28942\t\t\t0.214898\n53\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n54\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n55\t\tConvolutional\t36864\t\t25793\t\t\t0.300320\n56\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n57\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n58\t\tConvolutional\t36864\t\t27942\t\t\t0.242025\n59\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n60\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n61\t\tConvolutional\t36864\t\t29411\t\t\t0.202176\n62\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n63\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n64\t\tLinear\t\t640\t\t571\t\t\t0.107812\nTotal nonzero parameters: 210166\nTotal parameters: 274144\nTotal sparsity: 0.233374\n-------------------------------\n"
    }
   ],
   "source": [
    "print(\"-----Summary After pruning-----\")\n",
    "summary(net)\n",
    "print(\"-------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-30T03:30:21.345Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Complete 1 layers quantization...\nComplete 2 layers quantization...\nComplete 3 layers quantization...\nComplete 4 layers quantization...\nComplete 5 layers quantization...\nComplete 6 layers quantization...\nComplete 7 layers quantization...\nComplete 8 layers quantization...\nComplete 9 layers quantization...\nComplete 10 layers quantization...\nComplete 11 layers quantization...\nComplete 12 layers quantization...\nComplete 13 layers quantization...\nComplete 14 layers quantization...\nComplete 15 layers quantization...\nComplete 16 layers quantization...\nComplete 17 layers quantization...\nComplete 18 layers quantization...\nComplete 19 layers quantization...\nComplete 20 layers quantization...\nComplete 21 layers quantization...\nComplete 22 layers quantization...\nComplete 23 layers quantization...\n"
    }
   ],
   "source": [
    "net.load_state_dict(torch.load(\"net_after_pruning.pt\"))\n",
    "centers = quantize_whole_model(net, bits=5)\n",
    "np.save(\"codebook_resnet20.npy\", centers)\n",
    "torch.save(net.state_dict(), \"net_after_quantization.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-30T03:30:21.348Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Files already downloaded and verified\nTest Loss=0.3140, Test accuracy=0.9032\n"
    }
   ],
   "source": [
    "test(net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Huffman Coding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2020-09-30T03:30:21.352Z"
    },
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "3780836: '00101', 0.029059807: '00110', -0.038818043: '001110', 0.040651847: '0011110', 0.060874317: '001111100', 0.07053141: '001111101', 0.0772652: '001111110', 0.09354925: '0011111110', 0.11488338: '0011111111', 0.020194769: '0100', 0.014228703: '0101', -0.012899848: '011', 0.009442024: '1000', -0.028387519: '10010', 0.024220098: '10011', 0.01685347: '1010', -0.019990455: '1011', -0.01021429: '110', -0.016357133: '1110', -0.071333274: '11110000', -0.048703533: '11110001', 0.045993134: '1111001', 0.034784302: '111101', -0.024197405: '11111'}\nAverage storage for each parameter after Huffman Coding: 4.0839 bits\nComplete 2 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {0.029612372: '00000', 0.06172983: '0000100', 0.03705186: '0000101', 0.032565698: '000011', -0.01942347: '0001', 0.012650999: '0010', 0.015517792: '0011', -0.025008924: '0100', 0.01933244: '0101', 0.01718993: '0110', 0.014068632: '0111', 0.04896311: '1000000', 0.07565472: '10000010', 0.05567425: '10000011', -0.04138846: '100001', 0.04233037: '1000100', 0.10270627: '100010100', -0.10110932: '1000101010', 0.06650258: '1000101011', -0.05476157: '100010110', -0.0480985: '100010111', 0.010831565: '100011', -0.021946013: '1001', -0.017611021: '1010', -0.015564645: '1011', -0.013737056: '1100', -0.033990562: '11010', 0.021605702: '11011', -0.012258657: '11100', 0.026867213: '11101', 0.024053132: '11110', -0.028698266: '11111'}\nAverage storage for each parameter after Huffman Coding: 4.5066 bits\nComplete 3 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {0.026737368: '000', -0.027870676: '0010', -0.021614617: '0011', -0.0881405: '010000', -0.07239196: '0100010', 0.07695057: '0100011', 0.0142927915: '01001', -0.023570146: '0101', 0.031088805: '01100', 0.038369723: '01101', 0.047593117: '01110', 0.054432057: '011110', 0.060818546: '011111', -0.035977848: '1000', 0.022549937: '1001', -0.055918902: '10100', -0.044391982: '10101', -0.039985456: '1011', -0.031375103: '11000', 0.01950648: '11001', -0.06800159: '110100', -0.06348031: '110101', -0.051813938: '110110', -0.025141425: '110111', -0.019565169: '11100', -0.018286344: '11101', 0.035629094: '111100', 0.042338245: '111101', 0.049642347: '111110', 0.09281139: '1111110', -0.11568171: '11111110', -0.104398035: '11111111'}\nAverage storage for each parameter after Huffman Coding: 4.7413 bits\nComplete 4 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {-0.0104819145: '0000', -0.020464282: '0001', -0.018346874: '0010', 0.031774994: '001100', -0.032623358: '001101', 0.026833497: '00111', -0.011924451: '010', 0.015731473: '0110', 0.02222557: '01110', -0.025566624: '01111', 0.018822238: '1000', 0.011434376: '1001', -0.047980674: '1010000', -0.07960481: '1010001000', -0.06879256: '1010001001', -0.06242057: '101000101', -0.056788858: '1010001100', 0.06015314: '1010001101', 0.04679034: '101000111', -0.04342704: '1010010', 0.036873393: '1010011', -0.02299399: '10101', -0.016389923: '1011', -0.0149158165: '1100', -0.013566608: '1101', 0.013302442: '1110', 0.010057097: '11110', -0.028831467: '111110', 0.042408958: '11111100', 0.04932136: '111111010', 0.055164702: '111111011', -0.037797023: '1111111'}\nAverage storage for each parameter after Huffman Coding: 4.3739 bits\nComplete 5 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {-0.020328393: '0000', -0.0155658: '0001', 0.012400083: '001', -0.012824666: '0100', -0.014169486: '0101', 0.017039854: '0110', -0.036459845: '011100', 0.05126528: '011101000', 0.058002066: '011101001', 0.04301266: '01110101', 0.0365292: '0111011', -0.025576869: '01111', -0.011614898: '1000', 0.022152748: '10010', 0.025258524: '10011', -0.032577768: '101000', 0.033144478: '1010010', -0.06636871: '1010011000', -0.049306516: '1010011001', -0.04592563: '101001101', -0.042611964: '1010011100', 0.04669217: '1010011101', -0.040528428: '101001111', -0.018439448: '10101', 0.014589634: '1011', -0.016798733: '11000', 0.019488592: '11001', 0.010510147: '1101', -0.010201851: '1110', -0.02267141: '11110', -0.028771702: '111110', 0.029098576: '111111'}\nAverage storage for each parameter after Huffman Coding: 4.4319 bits\nComplete 6 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {-0.028678628: '0000', -0.040013514: '00010', 0.04284106: '000110', -0.047557067: '000111', 0.037533354: '00100', -0.0364208: '00101', -0.043703984: '001100', -0.05535367: '00110100', 0.05908665: '00110101', 0.05261218: '0011011', -0.013336798: '00111', 0.019844083: '0100', 0.022559611: '0101', -0.015079662: '0110', -0.02583438: '0111', 0.012965757: '10000', -0.03243289: '10001', 0.017621802: '1001', 0.026405683: '1010', 0.015163466: '1011', -0.022840992: '1100', -0.020339359: '1101', 0.031902775: '11100', 0.047319394: '111010', -0.08547493: '111011000', -0.070623904: '111011001', -0.062984996: '1110110100', 0.063175: '1110110101', 0.06828003: '1110110110', 0.08019809: '1110110111', -0.051272508: '1110111', -0.01726249: '1111'}\nAverage storage for each parameter after Huffman Coding: 4.4479 bits\nComplete 7 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {0.018354218: '000', -0.03375772: '00100', 0.046158545: '0010100', 0.04916052: '0010101', -0.045619134: '001011', 0.021616857: '0011', -0.017613528: '0100', -0.030143175: '01010', 0.05490884: '01011000', 0.068347484: '010110010', 0.07095385: '010110011', 0.04173096: '0101101', 0.0331591: '010111', -0.0128931375: '0110', -0.020010322: '0111', -0.0371614: '100000', 0.03696264: '100001', 0.028713958: '10001', -0.023429813: '1001', -0.014233887: '1010', 0.025001012: '10110', -0.040438242: '101110', -0.010479904: '1011110', -0.05801094: '101111100', -0.05108964: '1011111010', 0.064109065: '1011111011', -0.052373964: '10111111', -0.026919613: '11000', -0.011778404: '11001', -0.015741473: '1101', 0.015316177: '1110', 0.012474406: '1111'}\nAverage storage for each parameter after Huffman Coding: 4.4102 bits\nComplete 8 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {-0.021726007: '0000', 0.020132188: '0001', -0.017106486: '0010', -0.01853538: '0011', -0.026717043: '01000', -0.029424949: '01001', 0.023112733: '0101', 0.046228472: '0110000', -0.043762032: '0110001', 0.03401536: '011001', 0.030101623: '01101', -0.012256376: '0111', 0.013634994: '1000', -0.015644828: '1001', 0.017806517: '1010', -0.020017274: '10110', 0.026832787: '10111', -0.013844485: '1100', 0.015522716: '1101', -0.03302635: '111000', 0.037846792: '111001', 0.011893753: '11101', 0.05111452: '11110000', -0.079013556: '11110001000', -0.0695744: '11110001001', -0.05903321: '1111000101', 0.05614512: '1111000110', 0.059342455: '11110001110', 0.07096173: '11110001111', 0.042102866: '1111001', -0.037834123: '111101', -0.024076685: '11111'}\nAverage storage for each parameter after Huffman Coding: 4.5103 bits\nComplete 9 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {-0.018983008: '0000', 0.023783429: '0001', 0.013908577: '001', -0.02630662: '01000', 0.035649937: '010010', -0.031858794: '010011', -0.017374236: '0101', -0.045110766: '01100000', 0.04750901: '01100001', -0.040959604: '0110001', 0.03148143: '011001', -0.024339436: '01101', -0.022285243: '01110', 0.027789917: '01111', -0.014288595: '1000', -0.015775498: '1001', -0.011375364: '1010', 0.020011386: '1011', -0.01285611: '1100', -0.02063988: '11010', -0.028692508: '110110', -0.049229417: '110111000', 0.05397453: '11011100100', 0.066989705: '11011100101', -0.053499967: '11011100110', 0.061060555: '110111001110', -0.07927986: '1101110011110', 0.058349654: '1101110011111', 0.041977633: '11011101', -0.03580659: '1101111', 0.016700702: '1110', 0.011495066: '1111'}\nAverage storage for each parameter after Huffman Coding: 4.3144 bits\nComplete 10 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {-0.06923808: '0000000', -0.06332184: '0000001', -0.020270746: '000001', -0.0479293: '00001', -0.020061096: '000100', -0.020020995: '0001010', 0.108343035: '0001011', -0.01824024: '000110', 0.018126294: '000111', -0.031600196: '001', 0.023552468: '010', 0.053324915: '01100', 0.06538634: '011010', 0.109458834: '0110110', 0.13063297: '0110111', 0.026311569: '0111', 0.03033378: '1000', -0.022992155: '1001', 0.020200405: '10100', -0.056019194: '101010', -0.019643528: '101011', -0.041034635: '1011', 0.021588976: '11000', 0.042302836: '11001', 0.04655804: '110100', 0.14531043: '1101010', -0.1048287: '11010110', -0.08952031: '11010111', -0.03650447: '11011', -0.028527213: '11100', 0.036082942: '11101', -0.02528682: '1111'}\nAverage storage for each parameter after Huffman Coding: 4.4878 bits\nComplete 11 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {0.024848321: '00000', -0.02928622: '000010', -0.033044804: '0000110', -0.036819927: '00001110', -0.041135337: '000011110', 0.0440508: '000011111', -0.020346984: '0001', 0.018742243: '0010', -0.01807118: '0011', -0.014823263: '0100', -0.02592441: '01010', 0.021742458: '01011', -0.016305467: '0110', 0.015944913: '0111', -0.009623545: '1000', -0.013443575: '1001', -0.0121557275: '1010', -0.022891132: '10110', 0.02834125: '101110', 0.032884732: '1011110', 0.036863483: '10111110', 0.058269836: '10111111000', -0.051794387: '101111110010', -0.047411025: '101111110011', -0.045020208: '1011111101', 0.04772644: '10111111100', 0.050100368: '10111111101', 0.04108345: '1011111111', 0.00978696: '1100', 0.011450571: '1101', -0.010890816: '1110', 0.013480697: '1111'}\nAverage storage for each parameter after Huffman Coding: 4.2912 bits\nComplete 12 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {0.015634377: '0000', 0.019523222: '00010', 0.026990546: '000110', 0.03652296: '00011100', 0.03344981: '00011101', 0.03015456: '0001111', -0.009018701: '001', -0.019036558: '0100', -0.016548356: '0101', -0.010657468: '011', -0.021541914: '10000', 0.024217242: '100010', -0.028093625: '1000110', 0.043992266: '1000111000', -0.047485895: '100011100100', -0.042517837: '100011100101', -0.039815925: '10001110011', -0.037195604: '10001110100', 0.041557137: '10001110101', 0.0498378: '100011101100', 0.053301252: '100011101101', 0.054977443: '10001110111', -0.03176261: '10001111', 0.0087257475: '1001', 0.013614292: '1010', 0.011858335: '1011', 0.0102202445: '1100', 0.017501855: '11010', -0.02441105: '110110', 0.021728924: '110111', -0.014438831: '1110', -0.012429495: '1111'}\nAverage storage for each parameter after Huffman Coding: 4.1275 bits\nComplete 13 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {-0.013409772: '000', 0.018387534: '0010', -0.019593433: '0011', 0.016168501: '0100', 0.029624201: '010100', 0.03294654: '0101010', 0.04108555: '01010110', -0.037253443: '01010111', -0.024441386: '01011', -0.009367759: '0110', -0.017312855: '0111', 0.009615206: '1000', 0.020997427: '10010', -0.021809617: '10011', -0.015135442: '1010', -0.0105861565: '1011', 0.013711913: '1100', -0.01186761: '1101', 0.011438453: '1110', 0.026663179: '111100', -0.031704217: '1111010', 0.037775565: '11110110', 0.04600221: '111101110', 0.05082975: '11110111100', 0.053680718: '11110111101', -0.043781094: '11110111110', -0.060488176: '1111011111100', -0.052525897: '1111011111101', -0.04800707: '1111011111110', 0.07668703: '1111011111111', -0.027642038: '111110', 0.023785004: '111111'}\nAverage storage for each parameter after Huffman Coding: 4.2939 bits\nComplete 14 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {0.010046576: '000', -0.009051635: '001', 0.008209007: '010', 0.0144097125: '0110', 0.019770408: '01110', -0.01743278: '01111', -0.013081187: '1000', -0.020262394: '100100', -0.03161249: '100101000', 0.040031735: '100101001', 0.03257683: '10010101', -0.028357299: '10010110', 0.06869339: '10010111000', 0.046277694: '10010111001', 0.054680157: '100101110100', 0.060892563: '100101110101', -0.034426294: '10010111011', 0.036692616: '100101111', 0.022468518: '100110', 0.02552985: '1001110', -0.021606844: '1001111', -0.00790335: '1010', 0.012016611: '1011', -0.0116024865: '1100', -0.010241986: '1101', -0.01598289: '11100', 0.017061992: '11101', -0.014651248: '11110', -0.018795906: '111110', -0.025111534: '1111110', 0.028955244: '11111110', -0.022922076: '11111111'}\nAverage storage for each parameter after Huffman Coding: 4.2490 bits\nComplete 15 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {0.011195293: '000', 0.024259385: '00100', -0.022769647: '00101', 0.017173272: '0011', -0.025367886: '010000', 0.026872927: '010001', 0.021634314: '01001', -0.016329017: '0101', 0.015024935: '0110', -0.014671084: '0111', -0.013144936: '1000', -0.009297336: '1001', 0.049330715: '10100000000', 0.053557906: '10100000001', -0.043471694: '101000000100', 0.04577048: '101000000101', -0.039404813: '10100000011', 0.039348062: '101000001', -0.030709958: '10100001', -0.027937787: '1010001', 0.02944494: '1010010', 0.032315448: '10100110', 0.042106885: '1010011100', -0.03484647: '1010011101', 0.03589855: '101001111', 0.019376589: '10101', 0.009482333: '1011', -0.011737635: '1100', 0.013009125: '1101', -0.010516067: '1110', -0.020174136: '11110', -0.018103007: '11111'}\nAverage storage for each parameter after Huffman Coding: 4.3117 bits\nComplete 16 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {0.0083316015: '000', -0.009325449: '001', 0.025903786: '010000', -0.031018006: '01000100', 0.03245891: '01000101', 0.029102549: '0100011', 0.020506514: '01001', -0.014823647: '0101', 0.014041129: '0110', -0.01813606: '01110', 0.023081401: '011110', -0.022176329: '011111', -0.013283508: '1000', 0.018313944: '10010', -0.024730928: '1001100', 0.03563485: '100110100', 0.03903892: '10011010100', 0.043529443: '100110101010', 0.04615991: '10011010101100', 0.052744642: '10011010101101', 0.05588948: '10011010101110', 0.05950713: '10011010101111', -0.035263453: '1001101011', -0.027629346: '10011011', -0.020100472: '100111', -0.011844288: '1010', 0.012005603: '1011', -0.010593703: '1100', -0.016475784: '11010', 0.016167102: '11011', -0.00806595: '1110', 0.010107371: '1111'}\nAverage storage for each parameter after Huffman Coding: 4.2033 bits\nComplete 17 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {0.015678996: '0000', 0.012514256: '0001', -0.009757202: '001', -0.028379265: '010000', 0.042653278: '010001000', 0.049692873: '010001001', 0.04656854: '01000101', -0.03555668: '01000110', 0.03354252: '01000111', 0.017369132: '01001', 0.0140972715: '0101', -0.017595949: '0110', -0.015717234: '0111', 0.018984796: '10000', -0.032146297: '100010', -0.025871476: '100011', 0.011079339: '1001', -0.014348045: '1010', -0.012764303: '1011', 0.030239273: '1100000', 0.037107065: '11000010', -0.04814148: '1100001100', 0.03944667: '1100001101', -0.038152557: '110000111', 0.02051384: '110001', -0.022888036: '11001', 0.022719735: '110100', 0.0267322: '1101010', 0.025123665: '1101011', -0.020281127: '11011', 0.009642988: '1110', -0.011253769: '1111'}\nAverage storage for each parameter after Huffman Coding: 4.3682 bits\nComplete 18 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {-0.0090157995: '000', -0.0068432204: '001', -0.022783697: '010000', -0.040546943: '0100010000000', 0.04376465: '0100010000001', -0.037189752: '010001000001', 0.046086825: '010001000010', 0.047879037: '0100010000110', 0.055582207: '0100010000111', -0.034042586: '0100010001', 0.040023174: '0100010010', 0.03789414: '0100010011', 0.03408905: '010001010', -0.029470783: '010001011', 0.026874635: '0100011', 0.020492848: '010010', 0.030490717: '01001100', -0.025879601: '01001101', 0.023566408: '0100111', -0.0153404595: '0101', -0.007895559: '011', 0.010952466: '1000', 0.015298634: '10010', -0.017529456: '10011', -0.013384666: '1010', 0.009024657: '1011', -0.01988943: '110000', 0.017807454: '110001', 0.012967489: '11001', -0.011699556: '1101', -0.010278921: '1110', 0.007140372: '1111'}\nAverage storage for each parameter after Huffman Coding: 4.0295 bits\nComplete 19 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {0.012245121: '0000', -0.011658864: '00010', 0.016397364: '00011', -0.007091892: '001', 0.010229439: '0100', 0.020592883: '010100', 0.018654706: '010101', 0.014321935: '01011', 0.008414083: '0110', -0.008378531: '0111', 0.0045325616: '1000', 0.006913026: '1001', -0.00588126: '101', 0.022726294: '1100000', -0.013792333: '1100001', 0.027230188: '11000100', -0.018436735: '1100010100', 0.037199315: '110001010100', 0.042097688: '110001010101', 0.034651153: '11000101011', 0.029696673: '110001011', 0.024960823: '11000110', 0.040342517: '11000111000', 0.043805696: '110001110010', 0.048351526: '1100011100110', -0.020944702: '11000111001110', 0.04598639: '11000111001111', 0.032341264: '1100011101', -0.01611819: '110001111', -0.009821177: '11001', 0.0055895653: '1101', -0.0048165787: '111'}\nAverage storage for each parameter after Huffman Coding: 3.8859 bits\nComplete 20 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {-0.005368855: '000', 0.0043454897: '0010', 0.0036480092: '0011', -0.0071471496: '0100', -0.00917585: '01010', 0.0029057995: '01011', -0.0030348112: '011', -0.0045601744: '100', 0.005882717: '10100', -0.0117600355: '101010', 0.007880716: '101011', -0.0062313834: '1011', -0.003782434: '110', 0.009287402: '1110000', -0.018212171: '111000100', -0.02606871: '111000101000', 0.02542265: '1110001010010', -0.028170371: '11100010100110', 0.02157191: '11100010100111', 0.016964601: '11100010101', -0.02412087: '111000101100', -0.02159883: '111000101101', 0.015257839: '11100010111', 0.010879278: '11100011', -0.0131677985: '1110010', 0.013100961: '111001100', -0.0162243: '111001101', -0.014734888: '11100111', 0.005086362: '11101', -0.008145405: '11110', 0.0067781373: '111110', -0.010373592: '111111'}\nAverage storage for each parameter after Huffman Coding: 4.0017 bits\nComplete 21 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {-0.005627561: '000', 0.0043810955: '001', -0.009091051: '01000', -0.010495598: '010010', -0.012381877: '0100110', 0.016261036: '01001110', 0.020305814: '0100111100', 0.022781726: '0100111101', 0.018598968: '010011111', 0.0067858887: '0101', -0.0047107837: '011', -0.0038573903: '100', 0.009083607: '10100', 0.010575848: '101010', 0.012305591: '1010110', 0.014241021: '10101110', -0.014871892: '101011110', -0.025433628: '10101111100000', 0.030156264: '10101111100001', -0.021668058: '1010111110001', -0.023771308: '101011111001', 0.024915185: '10101111101', 0.028320368: '1010111111000', 0.026613904: '1010111111001', -0.019317651: '101011111101', -0.017336192: '10101111111', -0.006655215: '1011', 0.005897285: '1100', 0.0037149333: '1101', -0.0078239245: '11100', 0.007827161: '11101', 0.0051041706: '1111'}\nAverage storage for each parameter after Huffman Coding: 3.8950 bits\nComplete 22 layers for Huffman Coding...\nOriginal storage for each parameter: 5.0000 bits\nend {0.34285456: '0000', 0.3748563: '00010', 0.38896912: '00011', 0.40510085: '0010', 0.5046206: '00110', 0.61612546: '00111', 0.5321513: '0100', 0.63152105: '010100', 0.6894265: '010101', 0.6488304: '01011', 0.6680551: '01100', 0.72210455: '01101', 0.75511676: '011100', 0.8522223: '011101', -0.13056594: '01111', 0.27298906: '10000', 0.31044626: '10001', 0.36097237: '10010', 0.4421315: '10011', 0.5652422: '1010', 0.29420224: '1011', 0.47449812: '1100', -0.12082726: '110100', 0.12474118: '110101', -0.11836454: '11011', 0.1492377: '111000', 0.16928375: '111001', 0.19729802: '111010', 0.23331058: '111011', 0.22000712: '11110', 0.2516972: '111110', 0.32444096: '111111'}\nAverage storage for each parameter after Huffman Coding: 4.8652 bits\nComplete 23 layers for Huffman Coding...\n4.1031699396647605\n"
    }
   ],
   "source": [
    "frequency_map, encoding_map ,avbit= huffman_coding(net, centers)\n",
    "np.save(\"huffman_encoding\", encoding_map)\n",
    "np.save(\"huffman_freq\", frequency_map)\n",
    "print(avbit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": "Files already downloaded and verified\nTest Loss=0.3133, Test accuracy=0.9028\nLayer id\tType\t\tParameter\tNon-zero parameter\tSparsity(\\%)\n1\t\tConvolutional\t864\t\t218\t\t\t0.747685\n2\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n3\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n4\t\tConvolutional\t4608\t\t1120\t\t\t0.756944\n5\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n6\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n7\t\tConvolutional\t2304\t\t839\t\t\t0.635851\n8\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n9\t\tConvolutional\t512\t\t143\t\t\t0.720703\n10\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n11\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n12\t\tConvolutional\t2304\t\t829\t\t\t0.640191\n13\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n14\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n15\t\tConvolutional\t2304\t\t852\t\t\t0.630208\n16\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n17\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n18\t\tConvolutional\t2304\t\t806\t\t\t0.650174\n19\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n20\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n21\t\tConvolutional\t2304\t\t763\t\t\t0.668837\n22\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n23\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n24\t\tConvolutional\t4608\t\t1754\t\t\t0.619358\n25\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n26\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n27\t\tConvolutional\t9216\t\t3696\t\t\t0.598958\n28\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n29\t\tConvolutional\t512\t\t164\t\t\t0.679688\n30\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n31\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n32\t\tConvolutional\t9216\t\t3743\t\t\t0.593859\n33\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n34\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n35\t\tConvolutional\t9216\t\t3796\t\t\t0.588108\n36\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n37\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n38\t\tConvolutional\t9216\t\t3617\t\t\t0.607530\n39\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n40\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n41\t\tConvolutional\t9216\t\t3554\t\t\t0.614366\n42\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n43\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n44\t\tConvolutional\t18432\t\t7607\t\t\t0.587294\n45\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n46\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n47\t\tConvolutional\t36864\t\t15130\t\t\t0.589572\n48\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n49\t\tConvolutional\t2048\t\t823\t\t\t0.598145\n50\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n51\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n52\t\tConvolutional\t36864\t\t14655\t\t\t0.602458\n53\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n54\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n55\t\tConvolutional\t36864\t\t10750\t\t\t0.708388\n56\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n57\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n58\t\tConvolutional\t36864\t\t13473\t\t\t0.634521\n59\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n60\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n61\t\tConvolutional\t36864\t\t14645\t\t\t0.602729\n62\t\tBatchNorm\tN/A\t\tN/A\t\t\tN/A\n63\t\tReLU\t\tN/A\t\tN/A\t\t\tN/A\n64\t\tLinear\t\t640\t\t89\t\t\t0.860938\nTotal nonzero parameters: 103066\nTotal parameters: 274144\nTotal sparsity: 0.624044\n"
    }
   ],
   "source": [
    "test(net)\n",
    "summary(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "2.7.12-final"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}